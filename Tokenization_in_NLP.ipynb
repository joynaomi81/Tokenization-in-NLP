{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhJ4jrW5X9jXgrN+J1qg2H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joynaomi81/Tokenization-in-NLP/blob/main/Tokenization_in_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of breaking down a piece of text or unstructured data into smaller units or tokens,these tokens make it easier for a machine to process and understand human language. There are different types of tokenization, such as:\n",
        "\n",
        "1. Character tokenization\n",
        "2. Word tokenization\n",
        "3. Sentence tokenization\n",
        "4. Subword tokenization\n",
        "\n",
        "etc."
      ],
      "metadata": {
        "id": "AUxYKFVEsiu1"
      }
    },
    {
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssGd9cF-wB5r",
        "outputId": "4dab9f53-6003-4d4d-8391-34e3c43cc73c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence tokenization"
      ],
      "metadata": {
        "id": "uvnXZ0qMwmcs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-AqidK2eetqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5582984b-5752-49af-cd8f-e2964afaac87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We saw a large crowd at the bustop in the evening yesterday.',\n",
              " 'We sent him all his personal effects before March last year.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "text = \"We saw a large crowd at the bustop in the evening yesterday. We sent him all his personal effects before March last year.\"\n",
        "sent_tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word tokenization"
      ],
      "metadata": {
        "id": "nYznH5oDzmQV"
      }
    },
    {
      "source": [
        "# Word tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"We saw a large crowd at the bustop in the evening yesterday. We sent him all his personal effects before March last year.\"\n",
        "word_tokenize(text)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSN-pz4wzSp2",
        "outputId": "3f2458ee-7e5b-4b47-ed98-56beae3e4848"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We',\n",
              " 'saw',\n",
              " 'a',\n",
              " 'large',\n",
              " 'crowd',\n",
              " 'at',\n",
              " 'the',\n",
              " 'bustop',\n",
              " 'in',\n",
              " 'the',\n",
              " 'evening',\n",
              " 'yesterday',\n",
              " '.',\n",
              " 'We',\n",
              " 'sent',\n",
              " 'him',\n",
              " 'all',\n",
              " 'his',\n",
              " 'personal',\n",
              " 'effects',\n",
              " 'before',\n",
              " 'March',\n",
              " 'last',\n",
              " 'year',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Character tokenization"
      ],
      "metadata": {
        "id": "JiAIFjikz3uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def character_tokenization(text):\n",
        "    return list(text)\n",
        "tokenized_text = character_tokenization(text)\n",
        "print(\"text:\", tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mZjqNRqpldL",
        "outputId": "e62949c3-fe4a-4411-be77-2061c787d6c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: ['W', 'e', ' ', 's', 'a', 'w', ' ', 'a', ' ', 'l', 'a', 'r', 'g', 'e', ' ', 'c', 'r', 'o', 'w', 'd', ' ', 'a', 't', ' ', 't', 'h', 'e', ' ', 'b', 'u', 's', 't', 'o', 'p', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'e', 'v', 'e', 'n', 'i', 'n', 'g', ' ', 'y', 'e', 's', 't', 'e', 'r', 'd', 'a', 'y', '.', ' ', 'W', 'e', ' ', 's', 'e', 'n', 't', ' ', 'h', 'i', 'm', ' ', 'a', 'l', 'l', ' ', 'h', 'i', 's', ' ', 'p', 'e', 'r', 's', 'o', 'n', 'a', 'l', ' ', 'e', 'f', 'f', 'e', 'c', 't', 's', ' ', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'M', 'a', 'r', 'c', 'h', ' ', 'l', 'a', 's', 't', ' ', 'y', 'e', 'a', 'r', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subword tokenization"
      ],
      "metadata": {
        "id": "wTAPauPE1k0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLNetTokenizer\n",
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet/xlnet-base-cased\")"
      ],
      "metadata": {
        "id": "BT8h48WVplZI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(\"tokenization, unusal\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3dP5Y-YplVj",
        "outputId": "880b7690-8ca9-40e2-fe8f-e66e0c95890d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁token', 'ization', ',', '▁un', 'us', 'al']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}